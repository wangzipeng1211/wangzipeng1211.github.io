<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch 07——保存提取</title>
      <link href="/2022/08/17/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/07%E4%BF%9D%E5%AD%98%E6%8F%90%E5%8F%96/"/>
      <url>/2022/08/17/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/07%E4%BF%9D%E5%AD%98%E6%8F%90%E5%8F%96/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 06——快速搭建法</title>
      <link href="/2022/08/17/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/06%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%B3%95/"/>
      <url>/2022/08/17/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/06%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Method-1"><a href="#Method-1" class="headerlink" title="Method 1"></a>Method 1</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):    <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()    <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)  <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)  <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):    <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = torch.relu(self.hidden(x))  <span class="comment"># 激励函数(隐藏层的线性值)  ## Python function</span></span><br><span class="line">        x = self.predict(x)   <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net1 = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(net1)   <span class="comment"># net 的结构</span></span><br></pre></td></tr></table></figure><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/../../../../../img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/06/01.jpg"></p><h1 id="Method-2"><a href="#Method-2" class="headerlink" title="Method 2"></a>Method 2</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># method2</span></span><br><span class="line">net2 = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">2</span>, <span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(),                <span class="comment"># Python class</span></span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(net2)</span><br></pre></td></tr></table></figure><h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/../../../../../img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/06/02.jpg"></p><h1 id="对比-amp-总结"><a href="#对比-amp-总结" class="headerlink" title="对比&amp;总结"></a>对比&amp;总结</h1><p>很明显第二种搭建方法更快速，代码量也更少  </p><p>对比输出而言，net2 输出的更多，因为它将激励函数也一同纳入进去了, 但是在 net1 中, 激励函数实际上是在 forward() 功能中才被调用的。  </p><p>这也就说明了, 相比 net2, net1 的好处就是, 可以根据个人需要添加个性化的前向传播过程, 比如(RNN). 不过如果不需要七七八八的过程, 相信 net2 这种形式更适合。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL 05——基于模型的强化学习</title>
      <link href="/2022/08/16/RL/05%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
      <url>/2022/08/16/RL/05%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/../../../img/RL/05_RL(1).png"></p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL 04——基于动态规划的强化学习</title>
      <link href="/2022/08/15/RL/04%E5%9F%BA%E4%BA%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
      <url>/2022/08/15/RL/04%E5%9F%BA%E4%BA%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/../../../img/RL/04_RL.png"></p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 05——Classification(分类)</title>
      <link href="/2022/08/15/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/05Classification/"/>
      <url>/2022/08/15/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/05Classification/</url>
      
        <content type="html"><![CDATA[<h1 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h1 id="创造数据"><a href="#创造数据" class="headerlink" title="创造数据"></a>创造数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)         <span class="comment"># 数据的基本形态</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># 类型0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># 类型0 y data (tensor), shape=(100, )</span></span><br><span class="line">x1 = torch.normal(-<span class="number">2</span>*n_data, <span class="number">1</span>)     <span class="comment"># 类型1 x data (tensor), shape=(100, 1)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># 类型1 y data (tensor), shape=(100, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></span><br><span class="line">x = torch.cat((x0, x1), ).<span class="built_in">type</span>(torch.FloatTensor)  <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).<span class="built_in">type</span>(torch.LongTensor)    <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=y.data.numpy(), s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">&#x27;RdYlGn&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/../../../../../img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/05/01.jpg"></p><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):    <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()    <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)  <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)  <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):    <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = torch.relu(self.hidden(x))  <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)   <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(net)   <span class="comment"># net 的结构</span></span><br></pre></td></tr></table></figure><h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/../../../../../img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/05/02.jpg"></p><h1 id="Optimizer-and-Loss-function"><a href="#Optimizer-and-Loss-function" class="headerlink" title="Optimizer and Loss function"></a>Optimizer and Loss function</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()  <span class="comment"># 输出是概率 [0.1, 0.2]二分类</span></span><br></pre></td></tr></table></figure><h1 id="Training-and-Plotting"><a href="#Training-and-Plotting" class="headerlink" title="Training and Plotting"></a>Training and Plotting</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练步数</span></span><br><span class="line">    out = net(x)   <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)  <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值  ## 梯度降为0</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        <span class="comment"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></span><br><span class="line">        prediction = torch.<span class="built_in">max</span>(F.softmax(out), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">&#x27;RdYlGn&#x27;</span>)</span><br><span class="line">        accuracy = <span class="built_in">sum</span>(pred_y == target_y)/<span class="number">200.</span>  <span class="comment"># 预测中有多少和真实值一样</span></span><br><span class="line">        plt.text(<span class="number">1.5</span>, -<span class="number">4</span>, <span class="string">&#x27;Accuracy=%.2f&#x27;</span> % accuracy, fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">plt.ioff()  <span class="comment"># 停止画图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a>输出</h2><p><img src="https://media.giphy.com/media/mdp759Y6FzAhFjrxX4/giphy.gif"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 04——Regression(回归)</title>
      <link href="/2022/08/14/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/04Regression/"/>
      <url>/2022/08/14/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/04Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h1 id="创造数据"><a href="#创造数据" class="headerlink" title="创造数据"></a>创造数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># unsqueeze使一维变二维</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())</span><br><span class="line"></span><br><span class="line">x, y = Variable(x), Variable(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/04/01.jpg"></p><h1 id="构造模型"><a href="#构造模型" class="headerlink" title="构造模型"></a>构造模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):    <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()    <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)  <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)  <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):    <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = torch.relu(self.hidden(x))  <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)   <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(net)   <span class="comment"># net 的结构</span></span><br></pre></td></tr></table></figure><h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/04/02.jpg"></p><h1 id="Optimizer-and-Loss-function"><a href="#Optimizer-and-Loss-function" class="headerlink" title="Optimizer and Loss function"></a>Optimizer and Loss function</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">loss_func = torch.nn.MSELoss()           <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br></pre></td></tr></table></figure><h1 id="Training-and-Plotting"><a href="#Training-and-Plotting" class="headerlink" title="Training and Plotting"></a>Training and Plotting</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># training</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):  <span class="comment"># 训练步数</span></span><br><span class="line">    prediction = net(x)   <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)  <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值  ## 梯度降为0</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plotting</span></span><br><span class="line">    <span class="keyword">if</span> t%<span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>:<span class="number">15</span>, <span class="string">&#x27;color&#x27;</span>:<span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.3</span>)</span><br><span class="line">plt.ioff()</span><br></pre></td></tr></table></figure><h2 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a>输出</h2><p><img src="https://media.giphy.com/media/5x91dlIK2hmXHp6O42/giphy.gif"></p><p>由于数据为随机数据，所以两次结果呈现出来的loss有所不一样，但无伤大雅<br><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/04/03.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 03——Activation Function(激励函数)</title>
      <link href="/2022/08/14/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/03Activation/"/>
      <url>/2022/08/14/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/03Activation/</url>
      
        <content type="html"><![CDATA[<h1 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这  ## 新版本只需要import torch足矣</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt     <span class="comment"># 画图工具</span></span><br></pre></td></tr></table></figure><h1 id="创造数据"><a href="#创造数据" class="headerlink" title="创造数据"></a>创造数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 做一些假数据来观看图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">200</span>)  <span class="comment"># x data (tensor), shape=(200, 1)</span></span><br><span class="line">x = Variable(x)</span><br><span class="line">x_np = x.data.numpy()   <span class="comment"># 换成 numpy array, 出图时用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 几种常用的 激励函数</span></span><br><span class="line">y_relu = torch.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = torch.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = torch.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"><span class="comment"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 几种不常用的 激励函数</span></span><br><span class="line">y_relu6 = torch.nn.functional.relu6(x).data.numpy()</span><br><span class="line">y_elu = torch.nn.functional.elu(x).data.numpy()</span><br><span class="line">y_selu = torch.nn.functional.selu(x).data.numpy()</span><br><span class="line">y_celu = torch.nn.functional.celu(x).data.numpy()</span><br></pre></td></tr></table></figure><h1 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">plt.plot(x_np, y_relu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;sigmoid&#x27;</span>)  <span class="comment"># (0,1)</span></span><br><span class="line">plt.ylim((-<span class="number">0.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">plt.plot(x_np, y_tanh, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;tanh&#x27;</span>)        <span class="comment"># (-1,1)</span></span><br><span class="line">plt.ylim((-<span class="number">1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">plt.plot(x_np, y_softplus, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;softplus&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">plt.plot(x_np, y_relu6, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;relu6&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">plt.plot(x_np, y_elu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;elu&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">7</span>)</span><br><span class="line">plt.plot(x_np, y_selu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;selu&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>)</span><br><span class="line">plt.plot(x_np, y_celu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;celu&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/03/01.jpg"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://pytorch.org/docs/stable/nn.functional.html">torch.nn.functional 文档</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 02——Variable</title>
      <link href="/2022/08/13/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/02Variable/"/>
      <url>/2022/08/13/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/02Variable/</url>
      
        <content type="html"><![CDATA[<h1 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure><h1 id="创造Variable"><a href="#创造Variable" class="headerlink" title="创造Variable"></a>创造Variable</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])          <span class="comment"># tensor不可以反向传播</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)     <span class="comment"># variable可以反向传播</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="built_in">print</span>(variable)</span><br></pre></td></tr></table></figure><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/02/01.jpg"></p><h1 id="torch和variable分别求平均"><a href="#torch和variable分别求平均" class="headerlink" title="torch和variable分别求平均"></a>torch和variable分别求平均</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)</span><br><span class="line">v_out = torch.mean(variable*variable)</span><br><span class="line"><span class="built_in">print</span>(t_out)</span><br><span class="line"><span class="built_in">print</span>(v_out)</span><br></pre></td></tr></table></figure><h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/02/02.jpg"></p><h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><p>可以写出公式：<br>v_out &#x3D; 1&#x2F;4*sum(variable * variable)<br>所以：<br>d(v_out)&#x2F;d(variable) &#x3D; 1&#x2F;4 * 2 * variable &#x3D; variable&#x2F;2<br>又因为tensor形式不支持反向传递，所以t_out无法t_out.backward( )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v_out.backward()        <span class="comment"># 模拟 v_out 的误差反向传递</span></span><br><span class="line"><span class="built_in">print</span>(v_out)</span><br><span class="line"><span class="built_in">print</span>(variable)         <span class="comment"># 不是tensor形式，而是variable形式</span></span><br><span class="line"><span class="built_in">print</span>(variable.grad)    <span class="comment"># 初始 Variable 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(variable.data)    <span class="comment"># tensor形式</span></span><br><span class="line"><span class="built_in">print</span>(variable.data.numpy())  <span class="comment"># 转换为numpy</span></span><br></pre></td></tr></table></figure><h2 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/02/03.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 01——Numpy与Torch对比</title>
      <link href="/2022/08/13/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01Numpy%E4%B8%8ETorch%E5%AF%B9%E6%AF%94/"/>
      <url>/2022/08/13/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01Numpy%E4%B8%8ETorch%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<h1 id="下载Pytorch"><a href="#下载Pytorch" class="headerlink" title="下载Pytorch"></a>下载Pytorch</h1><p><a href="https://pytorch.org/get-started/locally/">Pytorch官网</a><br><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01/00.jpg"></p><h1 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch            <span class="comment"># 需要下载，没下载的可以到Pytorch官网上进行下载</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="创造数据"><a href="#创造数据" class="headerlink" title="创造数据"></a>创造数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numpy:&#x27;</span>, np_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch:&#x27;</span>, torch_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tensor2array:&#x27;</span>, tensor2array)</span><br></pre></td></tr></table></figure><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01/01.jpg"></p><h1 id="求绝对值、sin值、平均值"><a href="#求绝对值、sin值、平均值" class="headerlink" title="求绝对值、sin值、平均值"></a>求绝对值、sin值、平均值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">data = [-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 32bit  ## 转换成32位浮点tensor</span></span><br><span class="line"><span class="comment"># abs</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;abs&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numpy:&#x27;</span>, np.<span class="built_in">abs</span>(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch:&#x27;</span>,torch.<span class="built_in">abs</span>(tensor))</span><br><span class="line"></span><br><span class="line"><span class="comment"># sin  </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;sin&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numpy:&#x27;</span>, np.sin(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch:&#x27;</span>,torch.sin(tensor))</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numpy:&#x27;</span>, np.mean(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch:&#x27;</span>,torch.mean(tensor))</span><br></pre></td></tr></table></figure><h2 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01/02.jpg"></p><h1 id="求矩阵乘法"><a href="#求矩阵乘法" class="headerlink" title="求矩阵乘法"></a>求矩阵乘法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 32-bit floating point</span></span><br><span class="line">data = np.array(data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numpy:&#x27;</span>, np.matmul(data, data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;torch:&#x27;</span>, torch.mm(tensor, tensor))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numpy:&#x27;</span>, data.dot(data))</span><br></pre></td></tr></table></figure><h2 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01/03.jpg"></p><h1 id="类型的相互转换"><a href="#类型的相互转换" class="headerlink" title="类型的相互转换"></a>类型的相互转换</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="built_in">print</span>(tensor.long())            <span class="comment"># long类型 </span></span><br><span class="line"><span class="built_in">print</span>(tensor.short())           <span class="comment"># short类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">int</span>())             <span class="comment"># int类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.half())            <span class="comment"># 半精度浮点类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.double())          <span class="comment"># double类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">float</span>())           <span class="comment"># float类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.char())            <span class="comment"># char类型</span></span><br><span class="line"><span class="built_in">print</span>(tensor.byte())            <span class="comment"># byte类型</span></span><br></pre></td></tr></table></figure><h2 id="输出-3"><a href="#输出-3" class="headerlink" title="输出"></a>输出</h2><p><img src="/img/python/Pytorch/%E8%B7%9F%E7%9D%80%E8%8E%AB%E5%87%A1%E5%AD%A6pytorch/01/04.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Pytorch </category>
          
          <category> 跟着莫凡学pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL 03——马尔可夫决策过程</title>
      <link href="/2022/08/13/RL/03%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
      <url>/2022/08/13/RL/03%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/RL/03_RL.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL 02——探索与利用（多臂老虎机）</title>
      <link href="/2022/08/13/RL/02%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8_MAD/"/>
      <url>/2022/08/13/RL/02%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8_MAD/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/RL/02_RL.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL 01——RL简介</title>
      <link href="/2022/08/13/RL/01%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
      <url>/2022/08/13/RL/01%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/RL/01_RL.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL 00——RL分类</title>
      <link href="/2022/08/13/RL/00%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB/"/>
      <url>/2022/08/13/RL/00%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/RL/00_RL.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> DL </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex/Markdown 数学公式</title>
      <link href="/2022/08/13/Latex/Latex%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2022/08/13/Latex/Latex%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h1><p>$$<br>\delta,    \lambda      \<br>\Delta,    \Lambda      \<br>\Alpha,    \Beta        \<br>\phi,      \varphi      \<br>\epsilon,  \varepsilon  \<br>\mathbb{E}              \<br>\rho,\varrho            \<br>π<br>$$</p><p>Option + P &#x3D; π</p><p><img src="/img/Latex/00.jpg"></p><h1 id="上下标"><a href="#上下标" class="headerlink" title="上下标"></a>上下标</h1><p>$$<br>a^2, a_1\<br>x^{y+z}, p_{ij}, p_ij\<br>x_i,x_{\rm i}\<br>\text{A B}, \rm{A B} \<br>\text A B,  \rm A B  \<br>{\rm A} B\<br>$$</p><p><img src="/img/Latex/01.jpg"></p><h1 id="分式与根式"><a href="#分式与根式" class="headerlink" title="分式与根式"></a>分式与根式</h1><p>$$<br>\frac{1}{2},\frac 1 2,\<br>\frac 1 {x+y}\<br>\frac{\dfrac 1 x +1}{y+1}<br>$$</p><p>$$<br>\sqrt 2,\sqrt{x+y},\sqrt[3]{x+y}<br>$$</p><p><img src="/img/Latex/02.jpg"></p><h1 id="普通运算符"><a href="#普通运算符" class="headerlink" title="普通运算符"></a>普通运算符</h1><p>$$<br>+-\<br>\times,\cdot,\div\<br>\pm,\mp\<br>&lt;&gt;,\ge,\le,\gg,\ll,\ne,\approx,\equiv\<br>\cap,\cup,\in,\notin,\subset,\subseteq,\subsetneq,\subsetneqq,\varnothing\<br>\forall,\exists,\nexists\<br>\because,\therefore\<br>\mathbb R,\R,\Q,\N,\Z_+\<br>\mathcal F,\mathscr F\<br>$$</p><p><img src="/img/Latex/03.jpg"></p><p>$$<br>\cdots,\vdots,\ddots<br>$$</p><p><img src="/img/Latex/04.jpg"></p><p>$$<br>\infty,\partial,∂,\nabla,\propto,\degree<br>$$</p><p><img src="/img/Latex/05.jpg"></p><p>option+D &#x3D; ∂</p><p>$$<br>\sin x,\sec x,\cosh x\<br>\log_2 x, \ln x, \lg x\<br>\lim{x \to 0} \frac x {\sin x}\<br>\lim\limits_{x \to 0} \frac x {\sin x}\<br>\max x<br>$$</p><p><img src="/img/Latex/06.jpg"></p><p>$$<br>MSE(x)\<br>\text {MSE}(x)<br>$$</p><p><img src="/img/Latex/07.jpg"></p><h1 id="大型运算符"><a href="#大型运算符" class="headerlink" title="大型运算符"></a>大型运算符</h1><p>$$<br>\sum,\prod\<br>\sum_i,\sum_{i&#x3D;0}^N\<br>\frac{\sum\limits_{i&#x3D;1}^n x_i}{\prod\limits_{i&#x3D;1}^n x_i}<br>$$<br><img src="/img/Latex/08.jpg"></p><p>$$<br>\int,\iint,\iiint,\oint,\oiint\<br>\int_{-\infty}^0 f(x),\text dx<br>$$<br><img src="/img/Latex/09.jpg"></p><p>$$<br>a,a\<br>a\ a\<br>a\quad a\<br>a\qquad a\<br>$$</p><p><img src="/img/Latex/10.jpg"></p><h1 id="标注符号"><a href="#标注符号" class="headerlink" title="标注符号"></a>标注符号</h1><p>$$<br>\vec x, \overrightarrow {AB}\<br>\bar x, \overline{AB}\<br>$$</p><p><img src="/img/Latex/11.jpg"></p><h1 id="箭头"><a href="#箭头" class="headerlink" title="箭头"></a>箭头</h1><p>$$<br>\leftarrow,\rightarrow,\Rightarrow,\leftrightarrow,\longleftarrow<br>$$</p><p><img src="/img/Latex/12.jpg"></p><h1 id="括号与定界符"><a href="#括号与定界符" class="headerlink" title="括号与定界符"></a>括号与定界符</h1><p>$$<br>()[] { } \<br>\lceil,\rceil,\lfloor,\rfloor,|| \<br>\left(0,\frac 1 a \right]        \<br>\left.\frac {∂f}{∂x}\right|_{x&#x3D;0}<br>$$</p><p><img src="/img/Latex/13.jpg"></p><h1 id="多行公式"><a href="#多行公式" class="headerlink" title="多行公式"></a>多行公式</h1><p>$$<br>\begin{align}</p><p>a&amp;&#x3D;b+c+d\<br>&amp;&#x3D;e+f</p><p>\end{align}<br>$$</p><p><img src="/img/Latex/14.jpg"></p><h1 id="大括号"><a href="#大括号" class="headerlink" title="大括号"></a>大括号</h1><p>$$<br>f(x)&#x3D;</p><p>\begin{cases}</p><p>\sin x, &amp;-π\le x \le π\<br>0,&amp;\text {其他}</p><p>\end{cases}<br>$$</p><p><img src="/img/Latex/15.jpg"></p><h1 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h1><p>$$<br>\begin{matrix}</p><p>a &amp; b &amp; \cdots &amp; c \<br>\vdots&amp; \vdots &amp; \ddots &amp; \vdots \<br>e &amp; f &amp; \cdots &amp; g</p><p>\end{matrix}<br>$$<br><img src="/img/Latex/16.jpg"></p><p>$$<br>\begin{bmatrix}</p><p>a &amp; b &amp; \cdots &amp; c \<br>\vdots&amp; \vdots &amp; \ddots &amp; \vdots \<br>e &amp; f &amp; \cdots &amp; g</p><p>\end{bmatrix}<br>$$<br><img src="/img/Latex/17.jpg"></p><p>$$<br>\begin{pmatrix}</p><p>a &amp; b &amp; \cdots &amp; c \<br>\vdots&amp; \vdots &amp; \ddots &amp; \vdots \<br>e &amp; f &amp; \cdots &amp; g</p><p>\end{pmatrix}<br>$$<br><img src="/img/Latex/18.jpg"></p><p>$$<br>\begin{vmatrix}</p><p>a &amp; b &amp; \cdots &amp; c \<br>\vdots&amp; \vdots &amp; \ddots &amp; \vdots \<br>e &amp; f &amp; \cdots &amp; g</p><p>\end{vmatrix}<br>$$<br><img src="/img/Latex/19.jpg"></p><p>$$<br>\bf A, \bf B^{\rm T}<br>$$<br><img src="/img/Latex/20.jpg"></p><h1 id="实战演练"><a href="#实战演练" class="headerlink" title="实战演练"></a>实战演练</h1><p>$$<br>f(x) &#x3D; \frac 1 {\sqrt{2π} \sigma} {\rm e}^{-\frac{(x-\mu)^2} {2\sigma^2}} \<br>f(x) &#x3D; \frac 1 {\sqrt{2π} \sigma} \exp \left[{-\frac{(x-\mu)^2} {2\sigma^2}}\right]<br>$$<br><img src="/img/Latex/21.jpg"></p><p>$$<br>\lim \limits _{N\to \infty} P \left{ \left| \frac{I \left( \alpha_i \right)}{N} - H(s)\right| &lt; \varepsilon  \right} &#x3D;1<br>$$<br><img src="/img/Latex/22.jpg"></p><p>$$<br>x(n) &#x3D; \frac 1 {2π} \int_{-π} ^π X\left( \rm e ^{\rm j \omega }\right) \rm e ^{\rm j \omega n} , {\rm d} \omega<br>$$<br><img src="/img/Latex/23.jpg"></p><p>$$<br>\begin{align}</p><p>\vec B \left( \vec r\right) &amp;&#x3D; \frac {\mu_0} {4π} \oint_C \frac {I , {\rm d} \vec l \times \vec R}{R^3}\<br>&amp;&#x3D; \frac {\mu_0} {4π} \int_V \frac{\vec J_V \times \vec R}{R^3},{\rm d} V’</p><p>\end{align}<br>$$<br><img src="/img/Latex/24.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> Latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Help </tag>
            
            <tag> Latex </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>表头文件</title>
      <link href="/2022/08/12/help/01demo/"/>
      <url>/2022/08/12/help/01demo/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;hello world&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Help </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Help </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello-world</title>
      <link href="/2022/08/12/help/hello-world/"/>
      <url>/2022/08/12/help/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Help </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Help </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
